{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Summary\n",
    "\n",
    "This notebook is a web scraper that crawls Medium.com based on an initial seed user. The scraper loops through the graph of followers/followees (called leaders generally in this code), and creates 2 main data structures:\n",
    "1) A dictionary of source:target pairs that describes the connections between leaders and followers in medium\n",
    "2) A dictionary of nodes that contains each user examined and metadata of #of followers and #of leaders\n",
    "\n",
    "This code was written for 2 main purposes:\n",
    "1) To generate a list of users, whose articles will then be scraped in Step 2 for analysis and modeling in Step 3 of the project.\n",
    "2) To product source:target pairs in a format readable by d3 for the purposes of making a force-directed network graph of a given medium network\n",
    "\n",
    "Note: Medium.com does not product a comprehensive list of users or articles published, so this type of connection between users is the only way to get a large sample size on the site. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inital imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "import re\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Data Structures and seed iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Decalare the structures we'll be working in\n",
    "\n",
    "\n",
    "# #nodes = {node1:{followers:3, leaders:4},node2:{followers:12, leaders: 13},node3:{followers:2, leaders: 9}...}\n",
    "# nodes = {}\n",
    "\n",
    "# #iterator = [node1,node2,node3...]\n",
    "# iterator = []\n",
    "\n",
    "# #links_dict = {source: node1, target: f1},{source: node1, target: f2},{source: node2, target: f3}\n",
    "# links_dict = {'links':[]}\n",
    "\n",
    "\n",
    "# #LEGACY\n",
    "# # followers_dict = {node1:[f,f,f],node2:[f,f,f,f,f,f,f,f,],node3:[f,f,f]...}\n",
    "# followers_dict = {}\n",
    "\n",
    "# #leaders_dict = {node1:[l,l,l,l], node2:[l,l], node3:[l,l,1,1,1,1,1,1,l]...}\n",
    "# leaders_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://medium.com/@mmidzik']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seed the iterator\n",
    "iterator.append('https://medium.com/@mmidzik')\n",
    "iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define functions to scrape a person's overview page\n",
    "def fix_K_or_M(n):\n",
    "    if ' Follower' in str(n):\n",
    "        n = re.sub(' Follower','',n)\n",
    "        return int(n)\n",
    "    elif 'K' in str(n):\n",
    "        n = re.sub('K','',n)\n",
    "        n = float(n) * 1000\n",
    "        return int(n)\n",
    "    elif 'M' in str(n):\n",
    "        n = re.sub('M','',n)\n",
    "        n = float(n) * 1000000\n",
    "        return int(n)\n",
    "    else: \n",
    "        n = float(n)\n",
    "        return int(n)\n",
    "\n",
    "def count_followers(soup):\n",
    "    try: \n",
    "        followers = soup.find(attrs ={'data-action-value':'followers'})\n",
    "        follower_count = followers.text\n",
    "        follower_count = follower_count.replace(\" Followers\", \"\")\n",
    "        follower_count = fix_K_or_M(follower_count)\n",
    "        return follower_count\n",
    "    except AttributeError:\n",
    "        return 0\n",
    "        \n",
    "def count_leaders(soup):\n",
    "    try: \n",
    "        following = soup.find(attrs ={'data-action-value':'following'})\n",
    "        following_count= following.text\n",
    "        following_count = following_count.replace(\" Following\", \"\")\n",
    "        following_count = fix_K_or_M(following_count)\n",
    "        return following_count\n",
    "    except AttributeError:\n",
    "        return 0\n",
    "\n",
    "def get_name(soup):\n",
    "    name = soup.find('h1',{'class':'ui-h2 hero-title' })\n",
    "    return name.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scroll to the bottom of the followers page and get the soup html again\n",
    "\n",
    "def scroll_page(max_avatar_count):\n",
    "    i = 0\n",
    "    dicts = {}\n",
    "    stream = []\n",
    "    catch_error = []\n",
    "    while len(stream) < max_avatar_count:\n",
    "        try:\n",
    "            time.sleep(.4)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            stream = driver.find_elements_by_class_name('ui-captionStrong')\n",
    "            catch_error.append(len(stream))\n",
    "            if len(catch_error)>6:\n",
    "                #this is a super janky way to test for a timeout error\n",
    "                if catch_error[-2] == catch_error[-3] == catch_error[-4] == catch_error[-5] == catch_error[-6] == catch_error[-7]:\n",
    "                    break\n",
    "                    print('hung before end')\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "\n",
    "    innerHTML = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,\"lxml\")\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_save(item,name):\n",
    "    with open(name, 'wb') as picklefile:\n",
    "        pickle.dump(item, picklefile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followers_dict(url,soup,followers_dict,iterator):\n",
    "        followers = soup.find_all('a', {'class':'link u-baseColor--link avatar u-width60 u-marginRight20 u-flex0'})\n",
    "        res = []\n",
    "        for follower in followers:\n",
    "            follower = follower['href']\n",
    "            #should put in the node's url here\n",
    "            res.append(follower)\n",
    "            iterator.append(follower)\n",
    "        followers_dict[url] = res\n",
    "        return followers_dict\n",
    "\n",
    "def get_leaders_dict(url,soup,leaders_dict,iterator):\n",
    "        leaders = soup.find_all('a', {'class':'link u-baseColor--link avatar u-width60 u-marginRight20 u-flex0'})\n",
    "        res = []\n",
    "        for leader in leaders:\n",
    "            leader = leader['href']\n",
    "            #should put in the node's url here\n",
    "            res.append(leader)\n",
    "            iterator.append(leader)\n",
    "        leaders_dict[url] = res\n",
    "        return leaders_dict\n",
    "\n",
    "#links_dict = {source: node1, targer: f1},{follower: node1, leader: f2},{follower: node2, leader: f3}\n",
    "#source == follower\n",
    "#target == leader\n",
    "\n",
    "def add_follower_links(url, soup):\n",
    "    followers = soup.find_all('a', {'class':'link u-baseColor--link avatar u-width60 u-marginRight20 u-flex0'})\n",
    "    for follower in followers:\n",
    "        follower = follower['href']\n",
    "        iterator.append(follower)\n",
    "        res = {'source':follower, 'target':url}\n",
    "        links_dict['links'].append(res)\n",
    "        \n",
    "def add_leader_links(url, soup):\n",
    "    leaders = soup.find_all('a', {'class':'link u-baseColor--link avatar u-width60 u-marginRight20 u-flex0'})\n",
    "    for leader in leaders:\n",
    "        leader = leader['href']\n",
    "        iterator.append(leader)\n",
    "        res = {'source':url, 'target':leader}\n",
    "        links_dict['links'].append(res)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through nodes & followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/mayamidzik/tools/chromedriver\")\n",
    "recover_list = []\n",
    "\n",
    "while len(nodes) < 2000000:\n",
    "    url = iterator[i]\n",
    "    if (url not in nodes) & (url != 'https://medium.com/@MediumStaff'):\n",
    "        followers_url = url+ '/followers'\n",
    "        driver.get(followers_url)\n",
    "        innerHTML = driver.execute_script(\"return document.body.innerHTML\") #returns the inner HTML as a string\n",
    "        \n",
    "        soup = BeautifulSoup(innerHTML,\"lxml\")\n",
    "\n",
    "        followers_count = count_followers(soup)\n",
    "        leaders_count = count_leaders(soup)\n",
    "\n",
    "        node_dict ={url:{'followers':followers_count,'leaders':leaders_count}}\n",
    "        nodes.update(node_dict)\n",
    "        \n",
    "        recover_list.append(i)\n",
    "        \n",
    "        soup = scroll_page(followers_count)\n",
    "        get_followers_dict(url,soup,followers_dict,iterator)\n",
    "        add_follower_links(url,soup)\n",
    "        \n",
    "        leaders_url = url+ '/following'\n",
    "        driver.get(leaders_url)\n",
    "        \n",
    "        soup = scroll_page(leaders_count)\n",
    "        get_leaders_dict(url,soup,leaders_dict,iterator)\n",
    "        add_leader_links(url,soup)\n",
    "\n",
    "    \n",
    "    if len(nodes)%50 == 0:\n",
    "        pickle_save(links_dict,'links_dict2.pkl')\n",
    "        pickle_save(followers_dict,'followers_dict2.pkl')\n",
    "        pickle_save(leaders_dict,'leaders_dict2.pkl')\n",
    "        pickle_save(iterator, 'iterator2.pkl')\n",
    "        pickle_save(nodes, 'nodes2.pkl')\n",
    "        \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-pickle all outputs to be safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_save(links_dict,'links_dict2.pkl')\n",
    "pickle_save(followers_dict,'followers_dict2.pkl')\n",
    "pickle_save(leaders_dict,'leaders_dict2.pkl')\n",
    "pickle_save(iterator, 'iterator2.pkl')\n",
    "pickle_save(nodes, 'nodes2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[613, 614, 615]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recover_list[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat for D3 graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #getting into the right format for d3:\n",
    "# for key, value in nodes.items():\n",
    "#     nodes[key]['id'] = key\n",
    "# nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format needed for d3\n",
    "\n",
    "# {\n",
    "#     \"nodes\":[\n",
    "#         {\"id\": \"user\", \"group\": 1},\n",
    "#         {\"id\": \"user\", \"group\": 1},\n",
    "#         {\"id\": \"user\", \"group\": 1},\n",
    "#         {\"id\": \"user\", \"group\": 1},\n",
    "#     ],\n",
    "#     \"links\":[\n",
    "#         {\"source\": \"Napoleon\", \"target\": \"Myriel\", \"value\": 1},\n",
    "#         {\"source\": \"Mlle.Baptistine\", \"target\": \"Myriel\", \"value\": 8},\n",
    "#         {\"source\": \"Mme.Magloire\", \"target\": \"Myriel\", \"value\": 10},\n",
    "#         {\"source\": \"Mme.Magloire\", \"target\": \"Mlle.Baptistine\", \"value\": 6},\n",
    "#         {\"source\": \"CountessdeLo\", \"target\": \"Myriel\", \"value\": 1},\n",
    "#         {\"source\": \"Geborand\", \"target\": \"Myriel\", \"value\": 1},\n",
    "#         {\"source\": \"Champtercier\", \"target\": \"Myriel\", \"value\": 1}\n",
    "#     ]\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
